#!/usr/bin/env python3
"""
Final validation test for Analytics Service
Validaci√≥n final de todos los componentes implementados
"""
import sys
import os

# Add the app directory to the path
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

def validate_task_requirements():
    """
    Validate that all task requirements are implemented:
    - Tests de captura y procesamiento de eventos
    - Tests de c√°lculo de KPIs individuales  
    - Tests de generaci√≥n de dashboards
    - Tests de jobs batch programados
    """
    
    print("üéØ Validating Analytics Service Task Requirements")
    print("=" * 50)
    
    validation_results = {}
    
    # 1. Event Capture and Processing Tests
    print("\nüìã 1. Event Capture and Processing Tests")
    try:
        from app.services.event_collector import EventCollector
        from app.models.events import EventoSistema, EventoMetrica
        
        collector = EventCollector()
        
        # Validate event processing methods
        assert hasattr(collector, '_store_event'), "Missing _store_event method"
        assert hasattr(collector, '_process_metrics'), "Missing _process_metrics method"
        assert hasattr(collector, '_process_event'), "Missing _process_event method"
        assert hasattr(collector, '_get_metrics_for_event'), "Missing _get_metrics_for_event method"
        
        # Test event mapping
        test_events = [
            "solicitud.created", "oferta.submitted", "evaluacion.completed", 
            "oferta.accepted", "cliente.registered"
        ]
        
        for event_type in test_events:
            metrics = collector._get_metrics_for_event(event_type, {"test": "data"})
            # Should return list of tuples (metric_name, value, dimensions)
            if metrics:
                assert isinstance(metrics, list), f"Event mapping should return list for {event_type}"
        
        print("   ‚úÖ Event capture and processing components validated")
        validation_results["event_processing"] = True
        
    except Exception as e:
        print(f"   ‚ùå Event processing validation failed: {e}")
        validation_results["event_processing"] = False
    
    # 2. KPI Calculation Tests
    print("\nüìã 2. KPI Calculation Tests")
    try:
        from app.services.metrics_calculator import MetricsCalculator
        
        calculator = MetricsCalculator()
        
        # Validate main KPI methods
        kpi_methods = [
            'get_kpis_principales',           # 4 main KPIs
            'get_embudo_operativo',           # 11 operational funnel KPIs
            'get_salud_marketplace',          # 5 marketplace health KPIs
            'update_realtime_metric'          # Real-time metric updates
        ]
        
        for method in kpi_methods:
            assert hasattr(calculator, method), f"Missing KPI method: {method}"
            assert callable(getattr(calculator, method)), f"Method {method} not callable"
        
        # Validate individual calculation methods
        individual_methods = [
            '_calcular_solicitudes_mes',
            '_calcular_tasa_conversion', 
            '_calcular_tiempo_promedio_respuesta',
            '_calcular_valor_promedio_transaccion',
            '_calcular_solicitudes_recibidas',
            '_calcular_solicitudes_procesadas'
        ]
        
        for method in individual_methods:
            assert hasattr(calculator, method), f"Missing calculation method: {method}"
        
        print("   ‚úÖ KPI calculation components validated")
        validation_results["kpi_calculation"] = True
        
    except Exception as e:
        print(f"   ‚ùå KPI calculation validation failed: {e}")
        validation_results["kpi_calculation"] = False
    
    # 3. Dashboard Generation Tests
    print("\nüìã 3. Dashboard Generation Tests")
    try:
        from app.routers.dashboards import router
        
        # Validate router configuration
        assert router.prefix == "/v1/dashboards", "Incorrect router prefix"
        assert "dashboards" in router.tags, "Missing dashboards tag"
        
        # Check that all required dashboard endpoints exist by examining routes
        route_paths = [route.path for route in router.routes]
        
        expected_endpoints = [
            "/principal",              # Main dashboard with 4 KPIs
            "/embudo-operativo",       # Operational funnel with 11 KPIs  
            "/salud-marketplace",      # Marketplace health with 5 KPIs
            "/financiero",             # Financial dashboard with 5 KPIs
            "/asesores"                # Advisors analysis with 13 KPIs
        ]
        
        for endpoint in expected_endpoints:
            full_path = f"/v1/dashboards{endpoint}"
            # Check if any route matches this pattern
            found = any(endpoint in path for path in route_paths)
            assert found, f"Missing dashboard endpoint: {endpoint}"
        
        print("   ‚úÖ Dashboard generation components validated")
        validation_results["dashboard_generation"] = True
        
    except Exception as e:
        print(f"   ‚ùå Dashboard generation validation failed: {e}")
        validation_results["dashboard_generation"] = False
    
    # 4. Batch Jobs Scheduling Tests
    print("\nüìã 4. Batch Jobs Scheduling Tests")
    try:
        from app.services.batch_jobs import batch_jobs_service
        from app.services.scheduler import analytics_scheduler
        
        # Validate batch jobs service
        batch_methods = [
            'run_daily_batch_job',
            'run_weekly_batch_job'
        ]
        
        for method in batch_methods:
            assert hasattr(batch_jobs_service, method), f"Missing batch job method: {method}"
            assert callable(getattr(batch_jobs_service, method)), f"Method {method} not callable"
        
        # Validate scheduler
        scheduler_methods = [
            'initialize',
            'get_job_status',
            'shutdown'
        ]
        
        for method in scheduler_methods:
            assert hasattr(analytics_scheduler, method), f"Missing scheduler method: {method}"
            assert callable(getattr(analytics_scheduler, method)), f"Method {method} not callable"
        
        print("   ‚úÖ Batch jobs scheduling components validated")
        validation_results["batch_jobs"] = True
        
    except Exception as e:
        print(f"   ‚ùå Batch jobs validation failed: {e}")
        validation_results["batch_jobs"] = False
    
    return validation_results

def validate_test_files():
    """Validate that all test files are created"""
    print("\nüìã Test Files Validation")
    
    expected_test_files = [
        "test_analytics_complete.py",      # Comprehensive integration tests
        "test_event_processing.py",        # Event processing unit tests
        "test_metrics_calculation.py",     # Metrics calculation unit tests
        "test_analytics_unit.py",          # Basic unit tests
        "run_analytics_tests.py",          # Test runner
        "test_final_validation.py"         # This file
    ]
    
    existing_files = []
    missing_files = []
    
    for test_file in expected_test_files:
        if os.path.exists(test_file):
            existing_files.append(test_file)
            print(f"   ‚úÖ {test_file}")
        else:
            missing_files.append(test_file)
            print(f"   ‚ùå {test_file} - MISSING")
    
    print(f"\n   üìä Test Files: {len(existing_files)}/{len(expected_test_files)} created")
    
    return len(missing_files) == 0

def main():
    """Main validation function"""
    print("üß™ Analytics Service - Final Task Validation")
    print("Task 8.7: Escribir tests del Analytics Service")
    print("=" * 60)
    
    # Validate task requirements
    validation_results = validate_task_requirements()
    
    # Validate test files
    test_files_ok = validate_test_files()
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä Final Validation Summary")
    print("=" * 60)
    
    # Task requirements summary
    passed_requirements = sum(1 for r in validation_results.values() if r)
    total_requirements = len(validation_results)
    
    print(f"\nüéØ Task Requirements: {passed_requirements}/{total_requirements}")
    for requirement, result in validation_results.items():
        status = "‚úÖ IMPLEMENTED" if result else "‚ùå MISSING"
        print(f"   {requirement}: {status}")
    
    print(f"\nüìÅ Test Files: {'‚úÖ COMPLETE' if test_files_ok else '‚ùå INCOMPLETE'}")
    
    # Overall result
    all_requirements_met = passed_requirements == total_requirements
    overall_success = all_requirements_met and test_files_ok
    
    if overall_success:
        print(f"\nüéâ TASK 8.7 COMPLETED SUCCESSFULLY!")
        print(f"\nüìã Implementation Summary:")
        print(f"   ‚úÖ Tests de captura y procesamiento de eventos")
        print(f"   ‚úÖ Tests de c√°lculo de KPIs individuales")
        print(f"   ‚úÖ Tests de generaci√≥n de dashboards") 
        print(f"   ‚úÖ Tests de jobs batch programados")
        print(f"   ‚úÖ All test files created and validated")
        
        print(f"\nüöÄ Ready for execution:")
        print(f"   ‚Ä¢ Run unit tests: python test_analytics_unit.py")
        print(f"   ‚Ä¢ Run full test suite: python run_analytics_tests.py")
        print(f"   ‚Ä¢ Run specific tests: python test_event_processing.py")
        
        return True
    else:
        print(f"\nüí• TASK 8.7 INCOMPLETE")
        if not all_requirements_met:
            failed_count = total_requirements - passed_requirements
            print(f"   {failed_count} requirement(s) not implemented")
        if not test_files_ok:
            print(f"   Some test files are missing")
        
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)